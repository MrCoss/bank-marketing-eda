{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f823ca0f",
   "metadata": {},
   "source": [
    "# Bank Marketing Dataset – EDA & Feature Engineering Project\n",
    "\n",
    "This project performs a full Exploratory Data Analysis (EDA), data cleaning, transformation, and feature engineering pipeline on the **Bank Marketing Dataset**. It includes data visualization, handling of class imbalance, and generation of a comprehensive PDF report.\n",
    "\n",
    "---\n",
    "## Project Attribution\n",
    "\n",
    "This project is part of the **Skillfied Mentor Internship** program.\n",
    "\n",
    "- **Name:** Costas Antony Pinto  \n",
    "- **Program:** MCA – Artificial Intelligence & Machine Learning  \n",
    "- **University:** Manipal University Jaipur  \n",
    "- **Role:** Data Analyst Intern \n",
    "- **Project Title:** Bank Marketing Dataset – End-to-End EDA and Feature Engineering  \n",
    "- **Duration:** June-July 2025  \n",
    "- **Tools & Technologies:** Python, Pandas, Scikit-learn, SMOTE, ReportLab, Matplotlib, Seaborn, Missingno\n",
    "\n",
    "All tasks were self-coded and verified, demonstrating data cleaning, visualization, transformation, and feature engineering skills in a professional, production-ready format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2663fc",
   "metadata": {},
   "source": [
    "## Step 1: Environment & Directory Setup\n",
    "\n",
    "- Imported essential libraries for data processing, visualization, modeling, and report generation.\n",
    "- Created structured folders:\n",
    "  - `datasets/`: for raw and cleaned CSV files\n",
    "  - `plots/`: for generated graphs\n",
    "  - `data/`: for intermediate reports and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca04c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Environment & Directory Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "# Turn off unnecessary warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visual style for consistency\n",
    "sns.set(style='whitegrid', palette='Set2')\n",
    "\n",
    "# Define base and output directories\n",
    "BASE_DIR = r'G:\\My Drive\\MUJ MCA\\SKILLFIED MENTOR INTERNSHIP\\Banking Data Analysis'\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'datasets')\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, 'plots')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "# Create necessary folders\n",
    "for directory in [DATASET_DIR, PLOTS_DIR, DATA_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define input file path\n",
    "FILE_PATH = os.path.join(DATASET_DIR, 'bankmarketingdata.csv')\n",
    "\n",
    "# Define plot saving function\n",
    "def save_and_display(fig, plot_name, show=True):\n",
    "    save_path = os.path.join(PLOTS_DIR, f'{plot_name}.png')\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "    print(f'[SAVED] {plot_name}.png to plots/')\n",
    "\n",
    "print(\"Step 1 complete: Environment and folders are set up.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052f4e0",
   "metadata": {},
   "source": [
    "## Step 2: Load Raw Dataset & Initial Inspection\n",
    "\n",
    "- Used fallback logic to load dataset with `,` or `;` as delimiters.\n",
    "- Previewed:\n",
    "  - Dataset shape and data types\n",
    "  - Statistical summary\n",
    "  - Unique value count per column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Raw Dataset & Initial Inspection (Safe Fallback with Validation)\n",
    "\n",
    "def try_loading_csv(path, delimiter):\n",
    "    \"\"\"Attempts to load CSV with a given delimiter and checks if multiple columns are detected.\"\"\"\n",
    "    try:\n",
    "        df_temp = pd.read_csv(path, sep=delimiter)\n",
    "        if df_temp.shape[1] < 2:\n",
    "            return None  # Likely wrong delimiter\n",
    "        return df_temp\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    df = try_loading_csv(FILE_PATH, ',')  # Try comma first\n",
    "    if df is None:\n",
    "        df = try_loading_csv(FILE_PATH, ';')  # Fallback to semicolon\n",
    "\n",
    "    if df is None:\n",
    "        raise ValueError(\"Could not load dataset with common delimiters (',' or ';')\")\n",
    "\n",
    "    df_raw = df.copy()  # Backup\n",
    "\n",
    "    print(f\"[INFO] Dataset loaded. Shape: {df.shape}\\n\")\n",
    "\n",
    "    print(\"[PREVIEW] First 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\n[INFO] Dataset Info:\")\n",
    "    df.info()\n",
    "\n",
    "    print(\"\\n[INFO] Statistical Summary:\")\n",
    "    display(df.describe(include='all'))\n",
    "\n",
    "    print(\"\\n[INFO] Unique values in each column:\")\n",
    "    for col in df.columns:\n",
    "        print(f\" - {col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"[❌ ERROR] File not found: {FILE_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ ERROR] Failed to load or inspect dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9fbb9",
   "metadata": {},
   "source": [
    "## Step 3: Missing Values Visualization\n",
    "\n",
    "- Replaced `'unknown'` entries with `NaN`\n",
    "- Used `missingno` to visualize missing data:\n",
    "  - Bar plot\n",
    "  - Matrix plot\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32340d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Diagnose & Visualize Missing Values using missingno\n",
    "\n",
    "try:\n",
    "    # Replace 'unknown' with np.nan for accurate missing analysis\n",
    "    df.replace('unknown', np.nan, inplace=True)\n",
    "\n",
    "    # Plot 1: Missing Values Bar Plot\n",
    "    fig1 = plt.figure(figsize=(10, 4))\n",
    "    msno.bar(df, fontsize=12, color='royalblue')\n",
    "    plt.title('Missing Values - Bar Plot', fontsize=14)\n",
    "    save_and_display(fig1, 'missing_values_bar')\n",
    "\n",
    "    # Plot 2: Missing Values Matrix\n",
    "    fig2 = plt.figure(figsize=(10, 4))\n",
    "    msno.matrix(df, fontsize=12)\n",
    "    plt.title('Missing Values - Matrix Plot', fontsize=14)\n",
    "    save_and_display(fig2, 'missing_values_matrix')\n",
    "\n",
    "    # Print missing value count per column\n",
    "    print(\"\\n[INFO] Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to analyze or plot missing values: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b93782",
   "metadata": {},
   "source": [
    "## Step 4: Impute Missing Values\n",
    "\n",
    "- Categorical columns: filled with mode (most frequent value)\n",
    "- Numeric columns: filled with median\n",
    "- Verified no missing values remain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee3e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Impute Missing Values\n",
    "\n",
    "try:\n",
    "    # Iterate over all columns\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype == 'object':\n",
    "                # Impute with most frequent category\n",
    "                mode_val = df[col].mode()[0]\n",
    "                df[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"[IMPUTED] Categorical column '{col}' with mode: {mode_val}\")\n",
    "            else:\n",
    "                # Impute numeric with median\n",
    "                median_val = df[col].median()\n",
    "                df[col].fillna(median_val, inplace=True)\n",
    "                print(f\"[IMPUTED] Numeric column '{col}' with median: {median_val}\")\n",
    "\n",
    "    # Confirm no nulls remain\n",
    "    if df.isnull().sum().sum() == 0:\n",
    "        print(\"\\n[✅] Step 4 complete: All missing values handled.\")\n",
    "    else:\n",
    "        print(\"[⚠️] Some missing values remain.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed during missing value imputation: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b78d06",
   "metadata": {},
   "source": [
    "## Step 5: Drop Duplicates\n",
    "\n",
    "- Checked for duplicates\n",
    "- Removed and printed before-after shape\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Drop Duplicates\n",
    "\n",
    "try:\n",
    "    # Check for duplicates\n",
    "    initial_shape = df.shape\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "\n",
    "    if duplicate_count > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"[CLEANED] {duplicate_count} duplicate rows removed.\")\n",
    "    else:\n",
    "        print(\"[INFO] No duplicate rows found.\")\n",
    "\n",
    "    final_shape = df.shape\n",
    "    print(f\"[INFO] Dataset shape changed from {initial_shape} to {final_shape}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to check or remove duplicates: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056a893",
   "metadata": {},
   "source": [
    "## Step 6: Target Variable Distribution\n",
    "\n",
    "- Plotted the class balance of the target column `y`\n",
    "- Saved chart to `target_distribution.png`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Target Variable Distribution\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(data=df, x='y', palette='Set2')\n",
    "    plt.title('Target Variable Distribution (Subscription)', fontsize=14)\n",
    "    plt.xlabel('Subscribed', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    for p in plt.gca().patches:\n",
    "        plt.gca().annotate(f'{int(p.get_height())}', \n",
    "                           (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                           ha='center', va='bottom', fontsize=10)\n",
    "    save_and_display(fig, 'target_distribution')\n",
    "\n",
    "    print(\"\\n[INFO] Target Variable Value Counts:\")\n",
    "    print(df['y'].value_counts())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to plot target distribution: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e484e",
   "metadata": {},
   "source": [
    "## Step 7: Categorical Features vs Target\n",
    "\n",
    "- Barplots for:\n",
    "  - `job`, `education`, and `marital` vs `y`\n",
    "- Helped understand influence of features on target outcome\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83674ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Barplots of Categorical Features vs Target Variable\n",
    "\n",
    "try:\n",
    "    cat_features = ['job', 'marital', 'education']\n",
    "\n",
    "    for col in cat_features:\n",
    "        fig = plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(data=df, x=col, hue='y', palette='Set3')\n",
    "        plt.title(f'{col.title()} vs Subscription Outcome', fontsize=14)\n",
    "        plt.xlabel(col.title(), fontsize=12)\n",
    "        plt.ylabel('Count', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Subscribed', loc='upper right')\n",
    "        save_and_display(fig, f'barplot_{col}_y')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to create barplots for {col}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38aab2",
   "metadata": {},
   "source": [
    "## Step 8: Encode Categorical Variables\n",
    "\n",
    "- Encoded features using `LabelEncoder`\n",
    "- Target encoded to:\n",
    "  - `no` → 0\n",
    "  - `yes` → 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60988dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Encode Categorical Variables + Target\n",
    "\n",
    "try:\n",
    "    df_encoded = df.copy()\n",
    "    label_encoders = {}\n",
    "\n",
    "    # Encode categorical features (excluding 'y')\n",
    "    for col in df_encoded.select_dtypes(include='object').columns:\n",
    "        if col != 'y':\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "            label_encoders[col] = le\n",
    "            print(f\"[ENCODED] {col} using LabelEncoder.\")\n",
    "\n",
    "    # Encode target variable\n",
    "    df_encoded['y'] = df_encoded['y'].map({'no': 0, 'yes': 1})\n",
    "    print(\"[ENCODED] Target column 'y' mapped to 0 (no), 1 (yes).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to encode variables: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe514b5",
   "metadata": {},
   "source": [
    "## Step 9: Class Imbalance Before SMOTE\n",
    "\n",
    "- Plotted class distribution\n",
    "- Showed imbalance in `y` before applying SMOTE\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80317d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Class Imbalance Visualization (Before SMOTE)\n",
    "\n",
    "try:\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    df_encoded['y'].value_counts().plot(kind='bar', color=['tomato', 'lightblue'])\n",
    "    plt.title('Target Class Distribution (Before SMOTE)', fontsize=14)\n",
    "    plt.xlabel('Class (0 = No, 1 = Yes)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    for idx, val in enumerate(df_encoded['y'].value_counts()):\n",
    "        plt.text(idx, val + 200, str(val), ha='center', fontsize=10)\n",
    "    save_and_display(fig, 'class_distribution_before_smote')\n",
    "\n",
    "    print(\"\\n[INFO] Class counts before SMOTE:\")\n",
    "    print(df_encoded['y'].value_counts())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to plot class distribution: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1f3d3",
   "metadata": {},
   "source": [
    "## Step 10: Scaling + SMOTE Balancing\n",
    "\n",
    "- Standardized features using `StandardScaler`\n",
    "- Used `SMOTE` to balance class distribution\n",
    "- Visualized balanced classes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6fe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Scaling Features + Balancing with SMOTE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    # Separate features and target\n",
    "    X = df_encoded.drop('y', axis=1)\n",
    "    y = df_encoded['y']\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    print(\"[INFO] Features scaled using StandardScaler.\")\n",
    "\n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
    "    print(f\"[BALANCED] SMOTE applied. New class counts:\\n{pd.Series(y_resampled).value_counts()}\")\n",
    "\n",
    "    # Plot class distribution after SMOTE\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    pd.Series(y_resampled).value_counts().plot(kind='bar', color=['lightgreen', 'salmon'])\n",
    "    plt.title('Target Class Distribution (After SMOTE)', fontsize=14)\n",
    "    plt.xlabel('Class (0 = No, 1 = Yes)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    for idx, val in enumerate(pd.Series(y_resampled).value_counts()):\n",
    "        plt.text(idx, val + 200, str(val), ha='center', fontsize=10)\n",
    "    save_and_display(fig, 'class_distribution_after_smote')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed during scaling or SMOTE application: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4651d20f",
   "metadata": {},
   "source": [
    "## Step 11: Correlation Heatmap\n",
    "\n",
    "- Created correlation matrix\n",
    "- Saved heatmap to `correlation_heatmap.png`\n",
    "- Saved top 5 most correlated features to `top_corr_features.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf92322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Correlation Heatmap of Features\n",
    "\n",
    "try:\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df_encoded.corr(numeric_only=True)\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5, square=True,\n",
    "                cbar_kws={\"shrink\": .8}, annot_kws={\"size\": 8})\n",
    "    plt.title('Correlation Heatmap of Features', fontsize=16)\n",
    "    save_and_display(fig, 'correlation_heatmap')\n",
    "\n",
    "    # Show top 5 correlations with target\n",
    "    top_corr = corr_matrix['y'].drop('y').abs().sort_values(ascending=False).head(5)\n",
    "    print(\"\\n[INFO] Top 5 features most correlated with target:\")\n",
    "    print(top_corr)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to generate correlation heatmap: {e}\")\n",
    "\n",
    "    # Save top correlated features\n",
    "try:\n",
    "    corr_path = os.path.join(DATA_DIR, 'top_corr_features.csv')\n",
    "    top_corr.to_frame(name='Correlation_with_Target').to_csv(corr_path)\n",
    "    print(f\"[SAVED] Top correlated features → {corr_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ ERROR] Could not save top correlations: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cfd3f2",
   "metadata": {},
   "source": [
    "## Step 12: Feature Importance using Random Forest\n",
    "\n",
    "- Trained `RandomForestClassifier` to get feature importances\n",
    "- Visualized and saved results\n",
    "- Saved rankings in `feature_importance.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b357b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12: Feature Importance with Random Forest\n",
    "\n",
    "try:\n",
    "    # Train a Random Forest classifier on the original (non-resampled) data\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_scaled, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = rf_model.feature_importances_\n",
    "    features = df_encoded.drop('y', axis=1).columns\n",
    "    importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "    importance_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "    # Plot feature importance\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature', palette='viridis')\n",
    "    plt.title('Feature Importance - Random Forest', fontsize=14)\n",
    "    plt.xlabel('Importance Score', fontsize=12)\n",
    "    plt.ylabel('Feature', fontsize=12)\n",
    "    save_and_display(fig, 'feature_importance_rf')\n",
    "\n",
    "    # Print top 10 features\n",
    "    print(\"[INFO] Top 10 Features by Importance:\")\n",
    "    print(importance_df.head(10))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to compute or plot feature importance: {e}\")\n",
    "\n",
    "    # Save feature importance CSV\n",
    "try:\n",
    "    importance_path = os.path.join(DATA_DIR, 'feature_importance.csv')\n",
    "    importance_df.to_csv(importance_path, index=False)\n",
    "    print(f\"[SAVED] Feature importance → {importance_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ ERROR] Failed to save feature importance: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163257b6",
   "metadata": {},
   "source": [
    "## Step 13: Numeric Feature Distributions\n",
    "\n",
    "- Plotted:\n",
    "  - Histograms\n",
    "  - Boxplots\n",
    "  - Boxplots grouped by target class\n",
    "- Focused on `age`, `duration`, `campaign`, `pdays`, `previous`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c9eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Boxplots & Histograms for Key Numerical Features\n",
    "\n",
    "try:\n",
    "    numeric_cols = ['age', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # Histogram (Distribution)\n",
    "        fig1 = plt.figure(figsize=(7, 4))\n",
    "        sns.histplot(df[col], kde=True, bins=30, color='skyblue')\n",
    "        plt.title(f'Distribution of {col.title()}', fontsize=14)\n",
    "        plt.xlabel(col.title(), fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        save_and_display(fig1, f'distribution_{col}')\n",
    "\n",
    "        # Boxplot (Outliers)\n",
    "        fig2 = plt.figure(figsize=(7, 4))\n",
    "        sns.boxplot(x=df[col], color='lightcoral')\n",
    "        plt.title(f'Boxplot of {col.title()}', fontsize=14)\n",
    "        plt.xlabel(col.title(), fontsize=12)\n",
    "        save_and_display(fig2, f'boxplot_{col}')\n",
    "\n",
    "        # Boxplot by target class (if relevant)\n",
    "        fig3 = plt.figure(figsize=(7, 4))\n",
    "        sns.boxplot(data=df, x='y', y=col, palette='pastel')\n",
    "        plt.title(f'{col.title()} by Subscription Status', fontsize=14)\n",
    "        plt.xlabel('Subscribed (y)', fontsize=12)\n",
    "        plt.ylabel(col.title(), fontsize=12)\n",
    "        save_and_display(fig3, f'boxplot_{col}_by_y')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to generate plots for numeric column '{col}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc189ab7",
   "metadata": {},
   "source": [
    "## Step 14: Pairplot of Features\n",
    "\n",
    "- Sampled 400 rows for efficiency\n",
    "- Created pairplot of selected features colored by target class\n",
    "- Saved to `pairwise_relationships.png`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106965c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14: Pairplot of Selected Features (Sampled)\n",
    "\n",
    "try:\n",
    "    # Sample to speed up plotting\n",
    "    df_sample = df_encoded.sample(n=400, random_state=42)\n",
    "\n",
    "    # Select key features to compare\n",
    "    selected_features = ['age', 'duration', 'campaign', 'pdays', 'previous', 'y']\n",
    "\n",
    "    # Create pairplot\n",
    "    fig = sns.pairplot(df_sample[selected_features], hue='y', palette='husl', diag_kind='kde', corner=True)\n",
    "    pairplot_path = os.path.join(PLOTS_DIR, 'pairwise_relationships.png')\n",
    "    fig.savefig(pairplot_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"[SAVED] pairwise_relationships.png to plots/\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to generate pairplot: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f497d",
   "metadata": {},
   "source": [
    "## Step 15: Feature Engineering\n",
    "\n",
    "- Added:\n",
    "  - `age_group`: (young/adult/senior)\n",
    "  - `contacted_before`: binary\n",
    "  - `effective_contact`: based on contact history + success\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1dfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 15: Feature Engineering – Derived Columns\n",
    "\n",
    "try:\n",
    "    df_fe = df_encoded.copy()\n",
    "\n",
    "    # Feature 1: Age Grouping\n",
    "    df_fe['age_group'] = pd.cut(df_fe['age'],\n",
    "                                bins=[17, 30, 50, 100],\n",
    "                                labels=['young', 'adult', 'senior'])\n",
    "\n",
    "    # Feature 2: Was Contacted Before?\n",
    "    df_fe['contacted_before'] = np.where(df_fe['pdays'] == 999, 0, 1)\n",
    "\n",
    "    # Feature 3: Effective Contact (if previously contacted and successful outcome)\n",
    "    df_fe['effective_contact'] = np.where((df_fe['previous'] > 0) & (df_fe['contacted_before'] == 1), 1, 0)\n",
    "\n",
    "    print(\"[ADDED] Feature 'age_group' (young/adult/senior)\")\n",
    "    print(\"[ADDED] Feature 'contacted_before' (0/1)\")\n",
    "    print(\"[ADDED] Feature 'effective_contact' (0/1)\")\n",
    "\n",
    "    # Check for new features\n",
    "    print(\"\\n[INFO] New Features Preview:\")\n",
    "    display(df_fe[['age', 'age_group', 'pdays', 'contacted_before', 'effective_contact']].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed during feature engineering: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063ea8d",
   "metadata": {},
   "source": [
    "## Step 16: Skewness Detection & Log Transformation\n",
    "\n",
    "- Identified highly skewed numeric columns\n",
    "- Applied `np.log1p` transformation\n",
    "- Plotted before/after transformation\n",
    "- Saved to `skewness_report.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c8c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16: Skewness Detection & Log Transformation\n",
    "\n",
    "try:\n",
    "    df_skewed = df_fe.copy()\n",
    "    num_cols = df_skewed.select_dtypes(include=np.number).columns.drop('y')\n",
    "\n",
    "    print(\"[INFO] Skewness of Numerical Features:\")\n",
    "    skew_vals = df_skewed[num_cols].skew().sort_values(ascending=False)\n",
    "    print(skew_vals)\n",
    "\n",
    "    # Apply log1p (log(x + 1)) to highly skewed features (threshold > 1)\n",
    "    skewed_cols = skew_vals[skew_vals > 1].index.tolist()\n",
    "\n",
    "    for col in skewed_cols:\n",
    "        df_skewed[f'{col}_log'] = np.log1p(df_skewed[col])\n",
    "        print(f\"[TRANSFORMED] Applied log1p to '{col}' → '{col}_log'\")\n",
    "\n",
    "    # Plot one example before vs after\n",
    "    if skewed_cols:\n",
    "        col = skewed_cols[0]\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        sns.histplot(df_skewed[col], ax=axes[0], kde=True, color='orange')\n",
    "        axes[0].set_title(f'{col} - Original')\n",
    "\n",
    "        sns.histplot(df_skewed[f'{col}_log'], ax=axes[1], kde=True, color='green')\n",
    "        axes[1].set_title(f'{col}_log - After log1p')\n",
    "        plt.suptitle(f'Distribution Before vs After Log Transform for {col}', fontsize=14)\n",
    "\n",
    "        save_and_display(fig, f'log_transform_{col}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed during skewness check or log transformation: {e}\")\n",
    "\n",
    "    # Save skewness report\n",
    "try:\n",
    "    skew_path = os.path.join(DATA_DIR, 'skewness_report.csv')\n",
    "    skew_vals.to_frame(name='Skewness').to_csv(skew_path)\n",
    "    print(f\"[SAVED] Skewness report → {skew_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ ERROR] Could not save skewness report: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a88557",
   "metadata": {},
   "source": [
    "## Step 17: Multicollinearity Check using VIF\n",
    "\n",
    "- Calculated `Variance Inflation Factor (VIF)` for numeric features\n",
    "- Reported features with VIF > 5\n",
    "- Saved to `vif_report.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17: Multicollinearity Check using VIF\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "try:\n",
    "    df_vif = df_skewed.copy()\n",
    "\n",
    "    # Select numeric features only (excluding 'y')\n",
    "    vif_features = df_vif.select_dtypes(include=np.number).drop(columns=['y'], errors='ignore')\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    X_vif = pd.DataFrame(StandardScaler().fit_transform(vif_features), columns=vif_features.columns)\n",
    "\n",
    "    # Compute VIF scores\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = X_vif.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
    "    vif_data.sort_values(by='VIF', ascending=False, inplace=True)\n",
    "\n",
    "    # Display top VIFs\n",
    "    print(\"\\n[INFO] Variance Inflation Factor (VIF) Scores:\")\n",
    "    display(vif_data[vif_data['VIF'] > 5])\n",
    "\n",
    "    # Optional: Drop high VIF features (manually, based on domain)\n",
    "    # Example: drop_features = vif_data[vif_data['VIF'] > 10]['Feature'].tolist()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to compute VIF: {e}\")\n",
    "\n",
    "    # Save VIF report\n",
    "try:\n",
    "    vif_path = os.path.join(DATA_DIR, 'vif_report.csv')\n",
    "    vif_data.to_csv(vif_path, index=False)\n",
    "    print(f\"[SAVED] VIF report → {vif_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"[❌ ERROR] Could not save VIF report: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ce4bf",
   "metadata": {},
   "source": [
    "## Step 18: Save Cleaned Dataset\n",
    "\n",
    "- Final dataset saved to:\n",
    "  - `datasets/bankmarketing_cleaned.csv`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb4747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18: Save Final Cleaned Dataset to CSV\n",
    "\n",
    "try:\n",
    "    cleaned_data_path = os.path.join(DATASET_DIR, 'bankmarketing_cleaned.csv')\n",
    "    df_skewed.to_csv(cleaned_data_path, index=False)\n",
    "    print(f\"[✅] Cleaned dataset saved to: {cleaned_data_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to save cleaned dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea0fa8",
   "metadata": {},
   "source": [
    "## Step 19: EDA Summary Text Report\n",
    "\n",
    "- Included:\n",
    "  - Shape, column names, missing values\n",
    "  - Class distribution\n",
    "  - Feature importances\n",
    "  - Skewness\n",
    "  - VIF\n",
    "- Saved as `eda_summary.txt`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ef45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19: Save EDA Summary Report to TXT\n",
    "\n",
    "try:\n",
    "    summary_path = os.path.join(DATA_DIR, 'eda_summary.txt')\n",
    "    with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"🔍 EDA SUMMARY REPORT - BANK MARKETING DATASET\\n\")\n",
    "        f.write(\"=============================================\\n\\n\")\n",
    "\n",
    "        # Shape and columns\n",
    "        f.write(f\"[SHAPE] Final Dataset Shape: {df_skewed.shape}\\n\\n\")\n",
    "        f.write(\"[COLUMNS]\\n\")\n",
    "        f.write(\", \".join(df_skewed.columns) + \"\\n\\n\")\n",
    "\n",
    "        # Null check\n",
    "        f.write(\"[MISSING VALUES]\\n\")\n",
    "        nulls = df_skewed.isnull().sum()\n",
    "        f.write(nulls[nulls > 0].to_string() + \"\\n\\n\" if nulls.any() else \"No missing values.\\n\\n\")\n",
    "\n",
    "        # Class distribution\n",
    "        f.write(\"[TARGET DISTRIBUTION]\\n\")\n",
    "        f.write(df_skewed['y'].value_counts().to_string() + \"\\n\\n\")\n",
    "\n",
    "        # Feature importance (Top 10)\n",
    "        f.write(\"[FEATURE IMPORTANCE - Random Forest]\\n\")\n",
    "        f.write(importance_df.head(10).to_string(index=False) + \"\\n\\n\")\n",
    "\n",
    "        # Skewness summary\n",
    "        f.write(\"[SKEWNESS - Numeric Features]\\n\")\n",
    "        f.write(skew_vals.to_string() + \"\\n\\n\")\n",
    "\n",
    "        # VIF summary (Top 10)\n",
    "        f.write(\"[MULTICOLLINEARITY - VIF > 5]\\n\")\n",
    "        f.write(vif_data[vif_data['VIF'] > 5].to_string(index=False) + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Report auto-generated by EDA pipeline.\\n\")\n",
    "\n",
    "    print(f\"[✅] EDA summary saved to: {summary_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to write EDA summary report: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882b571",
   "metadata": {},
   "source": [
    "## Step 20: Auto-Generated PDF Report\n",
    "\n",
    "- Built a report using `reportlab`:\n",
    "  - Title, author, date\n",
    "  - Key visualizations\n",
    "  - Summary tables: correlation, feature importance, skewness, VIF\n",
    "- Saved to:\n",
    "  - `data/Bank_Marketing_EDA_Report.pdf`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f742f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, PageBreak\n",
    "from reportlab.lib.units import inch\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# === Define Paths ===\n",
    "BASE_DIR = r'G:\\My Drive\\MUJ MCA\\SKILLFIED MENTOR INTERNSHIP\\Banking Data Analysis'\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "PLOTS_DIR = os.path.join(BASE_DIR, 'plots')\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'datasets')\n",
    "PDF_REPORT = os.path.join(DATA_DIR, 'Bank_Marketing_EDA_Report.pdf')\n",
    "\n",
    "# === Load Data ===\n",
    "df = pd.read_csv(os.path.join(DATASET_DIR, 'bankmarketing_cleaned.csv'))\n",
    "feature_importance = pd.read_csv(os.path.join(DATA_DIR, 'feature_importance.csv'), index_col=0)\n",
    "vif_data = pd.read_csv(os.path.join(DATA_DIR, 'vif_report.csv'), index_col=0)\n",
    "skew_vals = pd.read_csv(os.path.join(DATA_DIR, 'skewness_report.csv'), index_col=0)\n",
    "top_corr = pd.read_csv(os.path.join(DATA_DIR, 'top_corr_features.csv'), index_col=0)\n",
    "\n",
    "# === Setup PDF Elements ===\n",
    "styles = getSampleStyleSheet()\n",
    "elements = []\n",
    "doc = SimpleDocTemplate(PDF_REPORT, pagesize=A4)\n",
    "\n",
    "# === Header ===\n",
    "elements.append(Paragraph(\"📊 Bank Marketing Dataset – EDA Report\", styles[\"Title\"]))\n",
    "elements.append(Paragraph(f\"Author: Costas Pinto\", styles[\"Normal\"]))\n",
    "elements.append(Paragraph(f\"Date: {datetime.now().strftime('%Y-%m-%d')}\", styles[\"Normal\"]))\n",
    "elements.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "# === Section 1: Dataset Info ===\n",
    "elements.append(Paragraph(\"1. Dataset Overview\", styles[\"Heading2\"]))\n",
    "elements.append(Paragraph(f\"Shape: {df.shape}\", styles[\"Normal\"]))\n",
    "elements.append(Paragraph(\"First 5 Columns: \" + ', '.join(df.columns[:5]), styles[\"Normal\"]))\n",
    "elements.append(PageBreak())\n",
    "\n",
    "# === Section 2: Plots ===\n",
    "def add_image(file_name, caption):\n",
    "    image_path = os.path.join(PLOTS_DIR, file_name)\n",
    "    if os.path.exists(image_path):\n",
    "        elements.append(Paragraph(caption, styles[\"Heading3\"]))\n",
    "        elements.append(Image(image_path, width=6*inch, height=3.5*inch))\n",
    "        elements.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "add_image(\"target_distribution.png\", \"2. Target Distribution\")\n",
    "add_image(\"barplot_job_y.png\", \"Job vs Target\")\n",
    "add_image(\"barplot_education_y.png\", \"Education vs Target\")\n",
    "add_image(\"barplot_marital_y.png\", \"Marital vs Target\")\n",
    "add_image(\"correlation_heatmap.png\", \"Correlation Heatmap\")\n",
    "add_image(\"feature_importance_rf.png\", \"Feature Importance (Random Forest)\")\n",
    "add_image(\"class_distribution_before_smote.png\", \"Before SMOTE\")\n",
    "add_image(\"class_distribution_after_smote.png\", \"After SMOTE\")\n",
    "add_image(\"log_transform_campaign.png\", \"Log Transformation on Campaign\")\n",
    "\n",
    "elements.append(PageBreak())\n",
    "\n",
    "# === Section 3: Feature Tables ===\n",
    "elements.append(Paragraph(\"3. Top Correlated Features\", styles[\"Heading2\"]))\n",
    "elements.append(Paragraph(top_corr.to_string(), styles[\"Code\"]))\n",
    "elements.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "elements.append(Paragraph(\"4. Top 10 Feature Importances\", styles[\"Heading2\"]))\n",
    "elements.append(Paragraph(feature_importance.head(10).to_string(index=False), styles[\"Code\"]))\n",
    "elements.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "elements.append(Paragraph(\"5. Skewness Values\", styles[\"Heading2\"]))\n",
    "elements.append(Paragraph(skew_vals.to_string(), styles[\"Code\"]))\n",
    "elements.append(Spacer(1, 0.2 * inch))\n",
    "\n",
    "elements.append(Paragraph(\"6. Features with VIF > 5\", styles[\"Heading2\"]))\n",
    "vif_filtered = vif_data[vif_data['VIF'] > 5]\n",
    "if not vif_filtered.empty:\n",
    "    elements.append(Paragraph(vif_filtered.to_string(index=False), styles[\"Code\"]))\n",
    "else:\n",
    "    elements.append(Paragraph(\"No features with VIF > 5\", styles[\"Normal\"]))\n",
    "\n",
    "elements.append(PageBreak())\n",
    "\n",
    "# === Footer ===\n",
    "elements.append(Paragraph(\"Report auto-generated using Python\", styles[\"Italic\"]))\n",
    "\n",
    "# === Build PDF ===\n",
    "doc.build(elements)\n",
    "print(f\"[PDF GENERATED using reportlab] → {PDF_REPORT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997e40a",
   "metadata": {},
   "source": [
    "### Final Project Conclusion\n",
    "\n",
    "**Project Title:** *Exploratory Data Analysis (EDA) on Bank Marketing Dataset*\n",
    "**Internship:** Skillfied Mentor Internship\n",
    "**Author:** *Costas Antony Pinto – MCA (AI & ML), Manipal University Jaipur*\n",
    "\n",
    "---\n",
    "\n",
    "### Project Goal\n",
    "\n",
    "To conduct a detailed exploratory data analysis (EDA) on the **Bank Marketing Dataset** to understand customer behavior and identify the most influential features that drive subscription to a term deposit.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Findings\n",
    "\n",
    "1. **Dataset Overview**:\n",
    "\n",
    "   * **Records:** 41,174\n",
    "   * **Features:** 31 (after feature engineering and transformations)\n",
    "   * Target variable: `y` (term deposit subscription)\n",
    "\n",
    "2. **Missing Values**:\n",
    "\n",
    "   * Placeholder `unknown` values were replaced with `NaN`.\n",
    "   * Imputation was done using mode (categorical) and median (numeric) strategies.\n",
    "   * Result: **No missing values remaining.**\n",
    "\n",
    "3. **Duplicates**:\n",
    "\n",
    "   * Removed **completely duplicated rows**, ensuring dataset integrity.\n",
    "\n",
    "4. **Target Distribution**:\n",
    "\n",
    "   * Highly imbalanced:\n",
    "\n",
    "     * **No:** \\~88.7%\n",
    "     * **Yes:** \\~11.3%\n",
    "\n",
    "5. **Class Balancing**:\n",
    "\n",
    "   * Applied **SMOTE** (Synthetic Minority Oversampling Technique) to balance the dataset before modeling.\n",
    "\n",
    "6. **Categorical Analysis**:\n",
    "\n",
    "   * Jobs like `student` and `retired` had a higher tendency to subscribe.\n",
    "   * Education and marital status also influenced subscription rates.\n",
    "\n",
    "7. **Feature Importance** (Random Forest):\n",
    "\n",
    "   * Top 5 important features:\n",
    "\n",
    "     1. `duration`\n",
    "     2. `nr.employed`\n",
    "     3. `pdays`\n",
    "     4. `euribor3m`\n",
    "     5. `emp.var.rate`\n",
    "\n",
    "8. **Correlation with Target**:\n",
    "\n",
    "   * Strongest correlation with `duration` (0.405), followed by `nr.employed` and `pdays`.\n",
    "\n",
    "9. **Skewness**:\n",
    "\n",
    "   * Features like `default`, `effective_contact`, and `campaign` showed heavy skew.\n",
    "   * Applied `log1p` transformation to reduce skewness.\n",
    "\n",
    "10. **Multicollinearity**:\n",
    "\n",
    "    * Very high VIF values observed for multiple features (e.g., `pdays`, `nr.employed`, etc.)\n",
    "    * Indicates **potential multicollinearity** which may require attention before modeling.\n",
    "\n",
    "11. **Feature Engineering**:\n",
    "\n",
    "    * Created new features:\n",
    "\n",
    "      * `age_group` (young, adult, senior)\n",
    "      * `contacted_before` (based on `pdays`)\n",
    "      * `effective_contact` (interaction between `pdays` and `previous`)\n",
    "\n",
    "---\n",
    "\n",
    "### Deliverables Generated\n",
    "\n",
    "* Cleaned dataset: `bankmarketing_cleaned.csv`\n",
    "* HTML report: `Bank_Marketing_EDA_Report.html`\n",
    "* PDF report: `Bank_Marketing_EDA_Report.pdf`\n",
    "* Supporting CSV files:\n",
    "\n",
    "  * `feature_importance.csv`\n",
    "  * `top_corr_features.csv`\n",
    "  * `vif_report.csv`\n",
    "  * `skewness_report.csv`\n",
    "* Summary text report: `eda_summary.txt`\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This EDA provided **a comprehensive understanding of the factors influencing customer subscription** in a banking marketing campaign. Key features such as `duration`, `pdays`, and economic indicators proved crucial. The project demonstrated the **importance of preprocessing**, **handling imbalanced data**, **feature engineering**, and **data visualization** in preparing for predictive modeling.\n",
    "\n",
    "> This structured, reproducible EDA pipeline can serve as a **solid foundation for machine learning models**, business insights, or further research into customer behavior analytics in financial domains.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
